## 什么是机器学习
机器学习是一个广义上被定义为基于**经验**(experience)提升性能或进行精准预测的计算方法，旨在设计**高效**和**准确**的预测**算法**(algorithm)
**经验**指的是学习器可利用的过去的信息
**样本复杂度**(sample complexity)是用来评估学习概念类所需的样本规模
由于机器学习的成功主要取决于采用的数据，所以本质与数据分析和统计相关。
## 机器学习的典型任务
**分类**(classification)：为每个事项制定类别。类别个数可以很少（图片分类）可以很多（OCR，文本分类）
**回归**(regression)：预测每个事项的实值。分类问题中不同类别之间没有距离的概念
**排序**(ranking)：按照某种准则将事项排序，例如网页搜索
**聚类**(clustering)：将事项集合划分为同质子集。常用于分析大数据集合
**降维**(dimensionality reduction)或者**流形学习**(manifold learning)：将事项的原始表示转化为低维表示，并保持原始表示的若干性质
## 学习阶段的术语
**样本**(examples)：用于学习或评估的数据事项或实例
**特征**(feature)：与样本关联的属性集合，通常表示为向量
**标签**(label)：分配给样本的数值或者类别
**超参数**(hyperparameter)：未被学习算法明确确定，算法输入中需要输入的自由参数
**训练样本**(training sample)：用于训练学习算法的样本
**验证样本**(validation sample)：用于调整学习算法参数的样本，针对于带标签的数据
**测试样本**(test sample)：用来评估学习算法性能的样本，测试样本要与训练以及验证样本分开，在学习阶段应该是**不可知**的
**损失函数**(loss function)：衡量预测标签和真实标签之间的差异或损失的函数，损失函数作为一种映射 $L:y \times y^{\prime} \rightarrow R_+$ ，其中所有标签集合记为$y$，可能的预测集合记为 $y^{\prime}$，大多数情况下$y=y^{\prime}$，并且损失函数是有界的，但是这些条件并不总是成立。常见的几种损失函数包括0-1损失和平方损失
**假设集**(hypothesis set)：将特征映射到标签集合 $y$ 的函数集合。假设集中的假设可以是将特征映射到不同集合 $y^{\prime}$ 的函数
学习过程的**典型阶段**的示意图：
![[Pasted image 20240229134753.png]]
## 学习情境
- **监督学习**(supervised learning)：学习器获得标签样本集作为训练数据，并对未见数据进行预测，这是与分类、回归和排序问题相关联的最常见的情景
- **无监督学习**(unsupervised learning)：学习器只获得无标签训练数据，并对未见数据进行预测，由于**标签样例**在该情形下通常是**不可获得**的，所以定量地评估学习器的性能是很困难的，**聚类**和**维数简约**是无监督学习问题的实例
- **半监督学习**(semi-supervised learning)：学习器获得的训练样本由标签数据和无标签数据组成，并对未见数据进行预测。半监督学习在**无标签数据容易获得**而**标签数据获得成本较高**的情景下是很常见的。借助可用的无标签数据的分布使得学习器取得比在监督情景下更好的性能
- **直推学习**(transductive inference)：与半监督情境一样，学习器获得标签训练样本以及无标签测试数据集合，但是直推学习的目标是仅对**特定测试数据**预测标签
- **在线学习**(on-line learning)：与其他情境相比，在线情境下学习**需要多轮**，同时训练和测试阶段混在一起。在每一轮，学习器获得一个无标签训练数据，对其作出预测之后，获得真实标签，并产生损失，在线情境下的目标是**最小化所有轮的累积损失**
- **强化学习**(reinforcement learning)：训练和测试阶段也是混合在一起的，为了收集信息，学习期主动地与环境进行交互，在一些情况下影响环境，并获得每个行动的**及时奖赏**，然而，环境如果不提供长期奖赏反馈，学习器必须在探索未知行动以获得更多信息和利用已收集信息之间进行选择，所以学习器面临着**探索还是利用**的困境
- **主动学习**(active learning)：学习器自适应地或者交互式地收集训练样本，通常以**询问专家**的方式请求新样本的标签，目标是利用更少的带标签样本达到与标准监督学习可比较的性能，主动学习常被用在标签获取成本高的实际应用中，例如计算生物学
## 泛化
机器学习本质上就是在研究**泛化**，从所有函数族的子集(称为**假设集**)中选择合适的函数，用于给出包括未见样本在内的所有样本的标签
在一个丰富的假设集中，学习器可能优先选择与训练样本一致(consistent)的函数或探测器作为假设，即对训练数据可以完全无误的划分，但如果假设集对应的函数族不够复杂，对训练数据产生误差在所难免
![[Pasted image 20240301234019.png]]
这两个训练样本是一样的，但是左侧的锯齿状曲线虽然在训练样本上表现很好，但是可能难以泛化到未见数据上，相反，右侧的决策面更加简单，虽然会误分类几个点，但可能泛化的更好。所以一般而言，训练样本上**表现最优**的预测器可能不是**全局最优**的，此外，从一个复杂函数族中选取的预测器本质上只是对训练数据标签的记忆，这**与泛化的理念相悖**。
**样本规模**和**假设复杂度**的折中在算法泛化中往往很重要。当**样本规模相对较小**时，从**过于复杂**的函数族中选择假设可能导致较差的泛化，即发生了**过拟合**(overfitting)。另一方面，当选择的函数族**过于简单**时，可能无法达到足够的预测精度，即发生了**欠拟合**(underfitting)

